{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "4061a1e78ec3443995228a014435a4eb",
      "41fbfb567ea34f7797279228ee912436",
      "5d6c6d96861c4ecf9868b9a625199520",
      "1240dd993d7f4920876b6c382a79c55e",
      "3a29078c52cc4f2d902a4520c8279c2b",
      "4200cd3aef634c1d8c91c99cb55f03f9",
      "796faed2908f4367a8e146811887fed6",
      "49139944779f40d290a5ca3c709512b2",
      "5e66d134403c4be294909aa231046f41",
      "6433677dae6f4f4b8769eae7dd4d9cf3",
      "43a06e6be3464793b353c6fa24d5f290",
      "422a0517bca4475dbf664b946a6a872f",
      "60791c25cd754cbcbe19314a84716012",
      "791adc9ef4804396adaa64a976837bd2",
      "bcc08971386542449d333539b611d552",
      "2b218939225a45b08d8d28ab70dff66b",
      "3d189b40de9849ffbbde78ed148a1bd7",
      "9859b44af8f04f1e8a270ebceaec0d9d",
      "4fa24100f1844b1e9132234541b7b85d",
      "1b011c7e70904b1c938b6e15f8ac5eaa",
      "97e1c038d63e423ebb4dc95dfc11e500",
      "12dc36cdad464ec0b20648209a2a185c",
      "b2f5554e9aec4dd8829bbe61a883e8e2",
      "5e8d95ab1d3848b5bbd1af5af070c7b5",
      "9974d2160e6044c192e58f1f629c5625",
      "8046b2ff1633437181a5512eb2355ca6",
      "6eda5881a9c646a5a0ab2c04a1c939e3",
      "83ec229fd02942a79540de9f69375b6d",
      "3b703772f6e8430fa90c622db894c123",
      "ba352fa6040b497ab3272f109d2ed487",
      "2784f824431249969dbf369e576c3cf7",
      "c0a2c9e36c3f4961883fef194527619f",
      "9a8e0d7aa24b489e9706df43600bc0ac",
      "37dc80299aa547e89eda45d7d474c8d4",
      "0243637b1c1c4917bad3fab54d29597c",
      "b91992fc3da74fb9a5ea9dd61d350dba",
      "c3d6a76c0eb74e849c8f4b8127fb8409",
      "330366c22a2a4220802f2a5284332c9c",
      "cac41c0463274a88b74ea1d5aedfda00",
      "5a4676eb78544afa83d43356d3af6a7a",
      "d7adb95d372a474495827c9cc9ca6196",
      "5df3b335274e46cdbf75c61e919eae84",
      "61b48531d34f4b0d863cfe4ef4b868a9",
      "fd510e131aa349d58e29201a31e29a79",
      "b096ddeae2d0468f8506862a12aca5eb",
      "185e065a08074b9cb90ea0cf581265f1",
      "99423f7009de40099dafc21e5f23a591",
      "ad13991a63454b6d913fd54f5b93c163",
      "7737f55881694adc8132f7eecaae983f",
      "2a95e6d3c9194bdfa15249d8ebdf0fc5",
      "463d544eff1244fda4cf2d8743dbf753",
      "e6077f86834741b3848aaacb36355b86",
      "14f3922be549437ebef7c823b7e3edec",
      "bb236513d501411ebb4053349fcb6582",
      "fe239f6347f94f7aa228523a516eda48"
     ]
    },
    "id": "Qdb4EQr4in6_",
    "outputId": "c2bfc632-2e43-4a7d-8b4e-ef0d500682df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Train samples: 50000 | Val samples: 10000 | Classes: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4061a1e78ec3443995228a014435a4eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "422a0517bca4475dbf664b946a6a872f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2049578459.py:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 0.4027 | Val Acc: 92.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2049578459.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/tmp/ipython-input-2049578459.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Fine-tuned Metrics ---\n",
      "Total params:        11,181,642\n",
      "Trainable params:    10,498,570\n",
      "Checkpoint size:     44.81 MB\n",
      "Latency (CPU, ms):   82.538\n",
      "Latency (GPU, ms): 4.789\n",
      "\n",
      "=== Running Post-Training Quantization (PTQ) ===\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2049578459.py:192: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  dynamic_quantized_model = quantize_dynamic(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f5554e9aec4dd8829bbe61a883e8e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Dynamic PTQ Metrics ---\n",
      "Model size: 44.79 MB\n",
      "Accuracy:   92.54%\n",
      "Latency:    85.139 ms\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2049578459.py:221: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  prepared_model = prepare_fx(model_to_quantize, {\"\": qconfig}, example_inputs=example_inputs)\n",
      "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  prepared = prepare(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating static quantizer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37dc80299aa547e89eda45d7d474c8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2049578459.py:231: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  static_quantized_model = convert_fx(prepared_model)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b096ddeae2d0468f8506862a12aca5eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2049578459.py:103: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/tmp/ipython-input-2049578459.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n",
      "/tmp/ipython-input-2049578459.py:119: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Static PTQ Metrics ---\n",
      "Model size: 11.31 MB\n",
      "Accuracy:   91.80%\n",
      "Latency:    31.596 ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Mount Drive for Checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Dataset Setup (CIFAR-10)\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = 224  # ResNet18 expects 224x224 input\n",
    "NUM_WORKERS = 8\n",
    "NUM_CLASSES = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "val_ds   = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    prefetch_factor=4, persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    prefetch_factor=4, persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)} | Classes: {NUM_CLASSES}\")\n",
    "\n",
    "#model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "#fine tuning the model(3layers)\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer3\" in name or \"layer4\" in name or \"fc\" in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "#replacing the fc in cifar-10\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "for param in model.fc.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.Adam(trainable_params, lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#helper fcns to determine metrics of models\n",
    "def count_parameters(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def save_model_state(m, path):\n",
    "    torch.save(m.state_dict(), path)\n",
    "    return os.path.getsize(path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        with torch.cuda.amp.autocast():\n",
    "            out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def latency_ms(model, device, n=50):\n",
    "    model.to(device).eval()\n",
    "    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device)\n",
    "    for _ in range(10):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        with torch.cuda.amp.autocast():\n",
    "            _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    return 1000.0 * (time.time() - t0) / n\n",
    "\n",
    "#training loop for model\n",
    "ckpt_dir = \"/content/drive/MyDrive/checkpoints\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "save_every = 200\n",
    "num_epochs = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for batch_idx, (xb, yb) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}\", leave=False)):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "        if (batch_idx + 1) % save_every == 0:\n",
    "            ckpt_path = f\"{ckpt_dir}/resnet18_epoch{epoch}_batch{batch_idx+1}.pth\"\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'batch_idx': batch_idx,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.item(),\n",
    "            }, ckpt_path)\n",
    "\n",
    "\n",
    "    epoch_loss = running_loss / len(train_ds)\n",
    "    val_acc = evaluate(model, val_loader, device)\n",
    "    print(f\"Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "#saving finetuned model to drive and determining baseline metrics\n",
    "final_path = os.path.join(ckpt_dir, \"finetuned_resnet18_cifar10_final.pth\")\n",
    "saved_size = save_model_state(model, final_path)\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "lat_cpu = latency_ms(model.cpu(), torch.device(\"cpu\"))\n",
    "lat_gpu = latency_ms(model.to(device), device) if torch.cuda.is_available() else None\n",
    "\n",
    "print(\"\\n--- Fine-tuned Metrics ---\")\n",
    "print(f\"Total params:        {total_params:,}\")\n",
    "print(f\"Trainable params:    {trainable_params:,}\")\n",
    "print(f\"Checkpoint size:     {saved_size/1e6:.2f} MB\")\n",
    "print(f\"Latency (CPU, ms):   {lat_cpu:.3f}\")\n",
    "if lat_gpu: print(f\"Latency (GPU, ms): {lat_gpu:.3f}\")\n",
    "\n",
    "#post training quantization of the model\n",
    "\n",
    "import copy\n",
    "from torch.ao.quantization import quantize_dynamic\n",
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization.qconfig import get_default_qconfig\n",
    "\n",
    "print(\"\\n=== Running Post-Training Quantization (PTQ) ===\\n\")\n",
    "\n",
    "#dynamic quantisation\n",
    "dynamic_quantized_model = quantize_dynamic(\n",
    "    copy.deepcopy(model.cpu()),\n",
    "    {nn.Linear},\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "\n",
    "dyn_path = os.path.join(ckpt_dir, \"resnet18_cifar10_PTQ_dynamic.pth\")\n",
    "torch.save(dynamic_quantized_model.state_dict(), dyn_path)\n",
    "\n",
    "dyn_size = os.path.getsize(dyn_path)\n",
    "dyn_acc = evaluate(dynamic_quantized_model, val_loader, torch.device(\"cpu\"))\n",
    "dyn_lat = latency_ms(dynamic_quantized_model, torch.device(\"cpu\"))\n",
    "\n",
    "print(f\"\\n--- Dynamic PTQ Metrics ---\")\n",
    "print(f\"Model size: {dyn_size/1e6:.2f} MB\")\n",
    "print(f\"Accuracy:   {dyn_acc*100:.2f}%\")\n",
    "print(f\"Latency:    {dyn_lat:.3f} ms\\n\")\n",
    "\n",
    "#static quantization\n",
    "\n",
    "qconfig = get_default_qconfig(\"fbgemm\")\n",
    "model_to_quantize = copy.deepcopy(model.cpu())\n",
    "model_to_quantize.eval()\n",
    "\n",
    "example_inputs = torch.randn(1, 3, IMG_SIZE, IMG_SIZE) # Add example input\n",
    "prepared_model = prepare_fx(model_to_quantize, {\"\": qconfig}, example_inputs=example_inputs)\n",
    "\n",
    "# calibration loop\n",
    "print(\"Calibrating static quantizer...\")\n",
    "with torch.no_grad():\n",
    "    for xb, yb in tqdm(val_loader, total=50):\n",
    "        prepared_model(xb)\n",
    "\n",
    "static_quantized_model = convert_fx(prepared_model)\n",
    "\n",
    "static_path = os.path.join(ckpt_dir, \"resnet18_cifar10_PTQ_static.pth\")\n",
    "torch.save(static_quantized_model.state_dict(), static_path)\n",
    "\n",
    "static_size = os.path.getsize(static_path)\n",
    "static_acc = evaluate(static_quantized_model, val_loader, torch.device(\"cpu\"))\n",
    "static_lat = latency_ms(static_quantized_model, torch.device(\"cpu\"))\n",
    "\n",
    "print(f\"\\n--- Static PTQ Metrics ---\")\n",
    "print(f\"Model size: {static_size/1e6:.2f} MB\")\n",
    "print(f\"Accuracy:   {static_acc*100:.2f}%\")\n",
    "print(f\"Latency:    {static_lat:.3f} ms\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 785,
     "referenced_widgets": [
      "a66d47c7b7f64406939b35bb33ad6da7",
      "0fdc3fd304974b24a5823e5f9b3097a6",
      "fdae87e8d6c24b02bd9019da0c16e215",
      "1a3208ba309146ddbe1f11f695568cd4",
      "6fffc96fc9d54ab6894eb0fffad97456",
      "bd55baca48544057bb610e58f8b79f27",
      "056a9745149e4c8d89465a529405a2e8",
      "94cc291c5a9c4d7a9f80b3336cea61c9",
      "c33eea1e0b9e4a52a052961295f432ad",
      "22f3b5284fbe4394a3eaa694a9285305",
      "2e1426e485764ef1a8ad799f049e466b",
      "508c2731e7864949bda654640fe3771d",
      "b65112f1ff164fff842ada7aeaa0d276",
      "4b918f2ae25749619e0fb5e72bb44a54",
      "12ebec7e00b2442f9b8f22c37b08155c",
      "d5bcf647556a4e308c845311913010df",
      "90b14a4e1d3a4f83b3bf86156e2ebd61",
      "29878ae6035b4ae58ef25585ae1ad14d",
      "9750794bd6df4db9aca8202bfdfa8dd5",
      "d00da66617af41b9a7d58b7d193553f9",
      "f4ccc9399d644d26ad49acf1117d5c68",
      "5baef1e10af1400091b14320c2a40e97",
      "4af2d292576245ca89f0a86caad122d4",
      "eb23c3178ae4404e9c2de37abe333991",
      "df4a6bbe174b44f5a99e25c1922de769",
      "e555354cc8734926a3571fba7a7e96a2",
      "bec54ae0fa7c480ba3f0bf55514dd767",
      "1e56d92ded8748da9e2fb77a8f660269",
      "6073529160af4f728b3ffa8d96595107",
      "bdd3846ea837402cba0adee46562b286",
      "9a02e0aa580047d3b15386a71ed9e979",
      "96614a6564a544bd8125883fc5ddc6ea",
      "22147a8e404943959d48b555d3d36427",
      "98bf16f66d7746d7b55652a03a24a3b2",
      "b4884346c88546efb0fa7d1ce102edc3",
      "d0c5594598ca483c8bb048f5611f674e",
      "2bc313bf227c42ceafed2369410ef93e",
      "869bd7b2d70e406d825e423f5d2b79f9",
      "b7aeafe64c3d45dcafff643a0166a41f",
      "95d781fcfb1947389442b85707021150",
      "958f85a9b7c540fb8ee3ebf5f5d83f15",
      "bc45d4b83b7742f6bd0ee9d8cb309005",
      "7e451166408d44a788e8797618ea3d6e",
      "950aa90c3369410c925ad3555164d233",
      "d51bd841fc424a54a6b46f30ad897a5b",
      "1951789f8868410cbd32d949606f96cc",
      "3e1575df571e46c1b63c5de474239c4e",
      "24268908aac6490c9d3809343219d7f5",
      "5175f4b35cda499fbf03ef7289ec9a36",
      "616ab4bdfb494bff9acc522973071b8c",
      "84325921c5dc4441b75b6c4cc8f78d77",
      "321aec99892f4a21952c0a5a038e6b07",
      "c0310aeb4068451992aaef321eca7878",
      "4aede4c9672e40b7919316e801a4620a",
      "f1e4f8703c6f4e6ea111883a07f4cc0e",
      "896fe0db9e55484d86ef1dd696e8d9d7",
      "f91ca74f33b24efe8f1b346d3f4cf794",
      "e2b1828fde5e46feaadc3575aab4e7f4",
      "6d4d8874767b45f1946950ecaad5b74c",
      "aba414dfc82045799a02f722344ac7d1",
      "c3c924d26be1468c8ca401ef4a191a30",
      "36b1258d248944ddb8cfc80a079fc6bc",
      "efe5604b466d4b2fa8a5a23c1122263e",
      "cc3f7b37f6db4afd84354052d8a81a9b",
      "06359996574a487fabcce75ddf83cf09",
      "eed04346fee841d391ec00da70e97d6e",
      "3f2d338a59c64c52b1114994d509ebb1",
      "187f6dacb87d4016be86e4248a8b1097",
      "946fe3346e654390b381cc86fab5c057",
      "c5688dcc076b4d6987f318e5ca6b80a1",
      "1861f39c1d1443d0a510cbe612cb8015",
      "d6671e81adbf4801a9c72cb310166c6e",
      "b9eba1bd98c846b0ba38e22fdb3f1b98",
      "0ca1d4176a7b4df88433f4ed055a48b8",
      "1cf0b2103efc49bf899e025b428407d1",
      "6e2c22b9025b47759a5fa945bda8f55c",
      "5cb47aaf5e544675849960cdee594fe0"
     ]
    },
    "id": "o3it6dsnkb0d",
    "outputId": "557e5378-4250-4133-b5ce-ce3a07b2a0d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:13<00:00, 12.3MB/s]\n",
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000 | Val samples: 10000 | Classes: 10\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 219MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Quantization Aware Training (QAT) on saved FP32 model ===\n",
      "\n",
      "Loading finetuned FP32 model from: /content/drive/MyDrive/finetuned_resnet18_cifar10_final.pth\n",
      "Loaded FP32 model successfully.\n",
      "Preparing model for QAT (fake quantization modules)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3276304653.py:139: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qat_prepared = prepare_qat_fx(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/quantize_fx.py:146: FutureWarning: Passing a QConfig dictionary to prepare is deprecated and will not be supported in a future version. Please pass in a QConfigMapping instead.\n",
      "  prepared = prepare(\n",
      "/usr/local/lib/python3.12/dist-packages/torch/ao/quantization/observer.py:246: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting QAT for 3 epochs on cuda...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a66d47c7b7f64406939b35bb33ad6da7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "508c2731e7864949bda654640fe3771d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Epoch 1 | Loss: 0.1723 | Fake-Quant Val Acc: 94.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4af2d292576245ca89f0a86caad122d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bf16f66d7746d7b55652a03a24a3b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Epoch 2 | Loss: 0.1171 | Fake-Quant Val Acc: 94.57%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d51bd841fc424a54a6b46f30ad897a5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "QAT Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896fe0db9e55484d86ef1dd696e8d9d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QAT Epoch 3 | Loss: 0.0864 | Fake-Quant Val Acc: 94.57%\n",
      "Converting QAT model to real INT8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3276304653.py:195: DeprecationWarning: torch.ao.quantization is deprecated and will be removed in 2.10. \n",
      "For migrations of users: \n",
      "1. Eager mode quantization (torch.ao.quantization.quantize, torch.ao.quantization.quantize_dynamic), please migrate to use torchao eager mode quantize_ API instead \n",
      "2. FX graph mode quantization (torch.ao.quantization.quantize_fx.prepare_fx,torch.ao.quantization.quantize_fx.convert_fx, please migrate to use torchao pt2e quantization API instead (prepare_pt2e, convert_pt2e) \n",
      "3. pt2e quantization has been migrated to torchao (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e) \n",
      "see https://github.com/pytorch/ao/issues/2259 for more details\n",
      "  qat_int8 = convert_fx(qat_prepared)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved INT8 QAT model → /content/drive/MyDrive/resnet18_cifar10_QAT_int8.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f2d338a59c64c52b1114994d509ebb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- QAT INT8 Metrics ---\n",
      "Model size: 11.31 MB\n",
      "Accuracy:   94.72%\n",
      "Latency:    34.752 ms\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Mount Drive for Checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "# Dataset Setup (CIFAR-10)\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = 224  # ResNet18 expects 224x224 input\n",
    "NUM_WORKERS = 8\n",
    "NUM_CLASSES = 10  # CIFAR-10 has 10 classes\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "val_ds   = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    prefetch_factor=4, persistent_workers=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=True,\n",
    "    prefetch_factor=4, persistent_workers=True\n",
    ")\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)} | Classes: {NUM_CLASSES}\")\n",
    "\n",
    "#model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "\n",
    "#utility functions\n",
    "def count_parameters(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def save_model_state(m, path):\n",
    "    torch.save(m.state_dict(), path)\n",
    "    return os.path.getsize(path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        # Removed torch.cuda.amp.autocast() to avoid float16 type issues with QAT\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def latency_ms(model, device, n=50):\n",
    "    model.to(device).eval()\n",
    "    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device)\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    return 1000.0 * (time.time() - t0) / n\n",
    "\n",
    "#qat after loading finetuned model from drive\n",
    "\n",
    "import copy\n",
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "from torch.ao.quantization.qconfig import get_default_qat_qconfig\n",
    "\n",
    "print(\"\\n=== Running Quantization Aware Training (QAT) on saved FP32 model ===\\n\")\n",
    "\n",
    "\n",
    "ckpt_path = \"/content/drive/MyDrive/finetuned_resnet18_cifar10_final.pth\"\n",
    "print(f\"Loading finetuned FP32 model from: {ckpt_path}\")\n",
    "\n",
    "qat_base_model = resnet18(weights=None)\n",
    "qat_base_model.fc = nn.Linear(qat_base_model.fc.in_features, 10)\n",
    "\n",
    "\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "qat_base_model.load_state_dict(state)\n",
    "qat_base_model.eval()\n",
    "\n",
    "print(\"Loaded FP32 model successfully.\")\n",
    "\n",
    "\n",
    "\n",
    "#preparing model for qat\n",
    "qat_qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "\n",
    "example_inputs = (torch.randn(1, 3, IMG_SIZE, IMG_SIZE),)\n",
    "\n",
    "print(\"Preparing model for QAT (fake quantization modules)...\")\n",
    "qat_prepared = prepare_qat_fx(\n",
    "    qat_base_model,\n",
    "    {\"\": qat_qconfig},\n",
    "    example_inputs=example_inputs\n",
    ")\n",
    "\n",
    "# Move to device for QAT training\n",
    "qat_prepared.to(device)\n",
    "qat_prepared.train()\n",
    "\n",
    "#finetuning the qat model\n",
    "\n",
    "num_epochs_qat = 3\n",
    "qat_lr = 5e-5\n",
    "qat_optimizer = torch.optim.Adam(\n",
    "    [p for p in qat_prepared.parameters() if p.requires_grad],\n",
    "    lr=qat_lr\n",
    ")\n",
    "qat_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"Starting QAT for {num_epochs_qat} epochs on {device}...\")\n",
    "\n",
    "for epoch in range(num_epochs_qat):\n",
    "    running_loss = 0.0\n",
    "    qat_prepared.train()\n",
    "\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"QAT Epoch {epoch+1}\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        qat_optimizer.zero_grad()\n",
    "        out = qat_prepared(xb)\n",
    "        loss = qat_criterion(out, yb)\n",
    "\n",
    "        loss.backward()\n",
    "        qat_optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_ds)\n",
    "\n",
    "\n",
    "    qat_prepared.eval()\n",
    "    with torch.no_grad():\n",
    "        val_acc_fake = evaluate(qat_prepared, val_loader, device)\n",
    "\n",
    "    print(f\"QAT Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Fake-Quant Val Acc: {val_acc_fake*100:.2f}%\")\n",
    "\n",
    "\n",
    "#convert to int8 model\n",
    "\n",
    "print(\"Converting QAT model to real INT8...\")\n",
    "qat_prepared.cpu()\n",
    "qat_int8 = convert_fx(qat_prepared)# ================== PTQ (DYNAMIC + STATIC) for Saved VGG16 ==================\n",
    "# Loads the baseline FP32 model, performs INT8 Dynamic and Static quantization,\n",
    "# evaluates Accuracy, Latency, and Model Size.\n",
    "\n",
    "import os, time\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.models import vgg16_bn, VGG16_BN_Weights\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 1. Environment + Dataset\n",
    "# ---------------------------------------------------\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "BASELINE_PATH = \"/content/vgg_baseline_outputs/vgg_baseline_fp32.pth\"\n",
    "\n",
    "OUT_DIR = \"/content/vgg_ptq_outputs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "IMG_SIZE    = 224\n",
    "BATCH_SIZE  = 32\n",
    "NUM_CLASSES = 10\n",
    "NUM_WORKERS = 2\n",
    "\n",
    "test_tf = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],\n",
    "                         [0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "test_ds  = datasets.CIFAR10(\"./data\", train=False, download=True, transform=test_tf)\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
    "\n",
    "print(\"Test samples:\", len(test_ds))\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 2. Load baseline model\n",
    "# ---------------------------------------------------\n",
    "def build_vgg(num_classes=10):\n",
    "    model = vgg16_bn(weights=VGG16_BN_Weights.IMAGENET1K_V1)\n",
    "    in_f = model.classifier[6].in_features\n",
    "    model.classifier[6] = nn.Linear(in_f, num_classes)\n",
    "    return model\n",
    "\n",
    "def load_baseline():\n",
    "    model = build_vgg()\n",
    "    state = torch.load(BASELINE_PATH, map_location=\"cpu\")\n",
    "    model.load_state_dict(state)\n",
    "    return model\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 3. Evaluation + latency + size functions\n",
    "# ---------------------------------------------------\n",
    "@torch.no_grad()\n",
    "def evaluate(model, device):\n",
    "    model.eval().to(device)\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in test_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb).argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def latency_ms(model, device, n=20):\n",
    "    model.eval().to(device)\n",
    "    x = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device)\n",
    "\n",
    "    # warmup\n",
    "    for _ in range(3):\n",
    "        _ = model(x)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        _ = model(x)\n",
    "        if device.type == \"cuda\":\n",
    "            torch.cuda.synchronize()\n",
    "    return (time.time() - t0) / n * 1000.0\n",
    "\n",
    "def model_size_mb(path):\n",
    "    return os.path.getsize(path) / (1024 * 1024)\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 4. Dynamic PTQ (with reduced FC option)\n",
    "# ---------------------------------------------------\n",
    "def dynamic_ptq(reduce_fc=True):\n",
    "    print(\"\\n[Dynamic PTQ] Running...\")\n",
    "\n",
    "    model = load_baseline()\n",
    "\n",
    "    # Optionally reduce classifier size to minimize FC overhead\n",
    "    if reduce_fc:\n",
    "        print(\" - Applying reduced FC: 4096 -> 512 -> 10\")\n",
    "        model.classifier = nn.Sequential(\n",
    "            nn.Linear(25088, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, NUM_CLASSES)\n",
    "        )\n",
    "\n",
    "    # Convert to CPU-only for quantization\n",
    "    model.cpu()\n",
    "\n",
    "    # Dynamic Quantization (weights \\u2192 int8)\n",
    "    qmodel = torch.quantization.quantize_dynamic(\n",
    "        model,\n",
    "        {nn.Linear},\n",
    "        dtype=torch.qint8\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    save_path = os.path.join(OUT_DIR, \"vgg16_dynamic_int8.pth\")\n",
    "    torch.save(qmodel.state_dict(), save_path)\n",
    "\n",
    "    # Metrics\n",
    "    # Dynamic quantized models typically run on CPU, as the quantized ops might not be available on GPU.\n",
    "    cpu_device = torch.device(\"cpu\")\n",
    "    acc = evaluate(qmodel, cpu_device)\n",
    "    lat = latency_ms(qmodel, cpu_device)\n",
    "    size = model_size_mb(save_path)\n",
    "\n",
    "    print(f\"Dynamic PTQ | Acc={acc*100:.2f}% | Latency={lat:.3f} ms | Size={size:.2f} MB\")\n",
    "    return acc, lat, size, save_path\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 5. Static PTQ (full-graph int8 conversion)\n",
    "# ---------------------------------------------------\n",
    "def static_ptq():\n",
    "    print(\"\\n[Static PTQ] Running...\")\n",
    "\n",
    "    model = load_baseline()\n",
    "    model.cpu()\n",
    "\n",
    "    # Fuse modules where possible\n",
    "    fused_model = torch.quantization.fuse_modules(\n",
    "        model,\n",
    "        [\n",
    "            [\"features.0\", \"features.1\", \"features.2\"],\n",
    "        ],\n",
    "        inplace=False\n",
    "    )\n",
    "\n",
    "    fused_model.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "    torch.quantization.prepare(fused_model, inplace=True)\n",
    "\n",
    "    # Calibration with 1 batch from test set\n",
    "    print(\" - Calibrating...\")\n",
    "    xb, _ = next(iter(test_loader))\n",
    "    fused_model(xb)\n",
    "\n",
    "    qmodel = torch.quantization.convert(fused_model, inplace=False)\n",
    "\n",
    "    # Save\n",
    "    save_path = os.path.join(OUT_DIR, \"vgg16_static_int8.pth\")\n",
    "    torch.save(qmodel.state_dict(), save_path)\n",
    "\n",
    "    # Metrics\n",
    "    acc = evaluate(qmodel, DEVICE)\n",
    "    lat = latency_ms(qmodel, DEVICE)\n",
    "    size = model_size_mb(save_path)\n",
    "\n",
    "    print(f\"Static PTQ | Acc={acc*100:.2f}% | Latency={lat:.3f} ms | Size={size:.2f} MB\")\n",
    "    return acc, lat, size, save_path\n",
    "\n",
    "# ---------------------------------------------------\n",
    "# 6. Run Everything\n",
    "# ---------------------------------------------------\n",
    "dyn_acc, dyn_lat, dyn_size, dyn_path = dynamic_ptq(reduce_fc=True)\n",
    "stat_acc, stat_lat, stat_size, stat_path = static_ptq()\n",
    "\n",
    "print(\"\\n================ FINAL PTQ RESULTS ================\")\n",
    "print(f\"Dynamic PTQ: Acc={dyn_acc*100:.2f}% | Lat={dyn_lat:.2f} ms | Size={dyn_size:.2f} MB\")\n",
    "print(f\"Static PTQ:  Acc={stat_acc*100:.2f}% | Lat={stat_lat:.2f} ms | Size={stat_size:.2f} MB\")\n",
    "print(\"===================================================\")\n",
    "\n",
    "int8_path = \"/content/drive/MyDrive/resnet18_cifar10_QAT_int8.pth\"\n",
    "torch.save(qat_int8.state_dict(), int8_path)\n",
    "\n",
    "print(f\"Saved INT8 QAT model \\u2192 {int8_path}\")\n",
    "\n",
    "#metric evaluation\n",
    "\n",
    "qat_int8.eval()\n",
    "int8_acc = evaluate(qat_int8, val_loader, torch.device(\"cpu\"))\n",
    "int8_lat = latency_ms(qat_int8, torch.device(\"cpu\"))\n",
    "int8_size = os.path.getsize(int8_path)\n",
    "\n",
    "print(\"\\n--- QAT INT8 Metrics ---\")\n",
    "print(f\"Model size: {int8_size/1e6:.2f} MB\")\n",
    "print(f\"Accuracy:   {int8_acc*100:.2f}%\")\n",
    "print(f\"Latency:    {int8_lat:.3f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343,
     "referenced_widgets": [
      "af2ea89dd3ef40aabcbf6d9a35399671",
      "053a71509e88406f9e9902feef277a3c",
      "e360190dd1304c77ba0dbb821ad8f29b",
      "a710cabc1ea54fae9ebc5111d235890f",
      "e2398472a9814857ac67da24e49334cd",
      "e478e279d18e42868890388b2cb8261c",
      "db9c1e21faba4548888b116168f7de16",
      "9f0d98c8a45549b78db6c6a757efe308",
      "a7c36ebbf5834f1997783c1e63bb7208",
      "9c77654b26e54c6ab5df6779b9d103fc",
      "4264a1688add4dc7bed3d428c203c4d6",
      "9b3ba1193ece4ea8a1fedb2913597f66",
      "4eeeef34cbd44e008870886f61cad1f3",
      "cbe36c220cf643e89be6fb08b94fc803",
      "09902e55962548c59613b0fd23ccc575",
      "c4c661581c824b59b03c242586675d3a",
      "e56187aa67fd4c14bbab3ba3e5bb5fd9",
      "d7aa767925d449119ce0b478ae6db36a",
      "a6a916d5d69649a984c981b6621ec2b2",
      "125043e6a4d04c1aaa53c711779728ba",
      "c52cdc145bee440faf263c99b5af6b1c",
      "344f1ea255cf423bbca27fe44ad1b06b",
      "a4ecad083bf048dd87ee3cde7333581e",
      "ca6306deb10f4764a84364c0911751bd",
      "60f12fc2d00b4a84a7c6b5e1f898f43e",
      "16ad1ff5a30148d998588c2e0956799e",
      "d5860844d0fc4f18ae81ec5776052ad0",
      "279da85867e74fcb955317828333f2eb",
      "ebfed10916a041f483a3bd70510c24ed",
      "f2d1672205544c609587769bdb2aaece",
      "4d0345ebf3514ffe9304caa7a8df7e4d",
      "2929bd65cc3540488cbb0a0c4ad96497",
      "3ab42bc97422441a900060f7465747f4",
      "4ab6d4ba2aaa46a5b94e2db102faa824",
      "07db495fb75f451a9aba18fcf4476eb9",
      "c99791af8499424fb747c71e1ce20e6f",
      "80a57b58597142bd94815cf057db4ac3",
      "b6e1d0a112b1405e8cb2b7ea1013d5eb",
      "e4b3b82037ba4746b703789f060aff4c",
      "bca1f49e38654641a67b1e7b572580c3",
      "35d32c9b658c45ce95045fc94b3eadc3",
      "b0bd238436db4a929c5fec1b0d86bb1c",
      "c45aa22204da4e1c8e7233a36c0a93b2",
      "3e12fb88c4b74f268322c5b22c79b4b8",
      "a53ba10f19334b9c8d9cdde4b40725bd",
      "f98b37518538423ea31ab32ae139e17c",
      "93b46f92af2d4ddaac68a32e048351ed",
      "3d023e4c9e5f46eb9af5722d94cda6d0",
      "c235a031a0234885bd04419351233dc3",
      "ffc60674a74941d0a7a8ca7a1374b0dc",
      "80a0c3dcaf1e41129bd88015e15fb97d",
      "011419f1fa3140ebbb200bcd461876c3",
      "02b4fec75ff643a2b4b4683dd83f2c9b",
      "de5d9a01ee104076a12cf5292cafd010",
      "698a37dc7f7445e8b910a9b307f0a523",
      "2688bc814f1e41a89dfbe52c9904be53",
      "52dc955989564cd6baae3e37ba437982",
      "94ababab446046a8a405629e6b3c9774",
      "ac117230ecca4c3faeeb795d1792651e",
      "db1f931b0f5748dd8b58fa01599a379d",
      "3eb65d9e6b1d4ffb81cb99e67db26a1a",
      "3c80a16a9d6a4f449293db6a2a7edc71",
      "a0f8385e50224aac96a7289bc6bfea84",
      "d9b065297224431087515ae19692ed5e",
      "cabda71aeeb9410d9c4c3a082c5496e5",
      "cbfafabd501f4dd68ca475340b1d5bb6",
      "9222483f62f544e5a590c35a3e6fa63a",
      "80997d3dec1e4a8e8b3a1a9cbcb4a0df",
      "89b9ba12424b490d93ed66e04e125b11",
      "05ce4bb1da7b4c3fb204935db6dd6939",
      "ca19bd9f04734654b899f7c753c77049",
      "a568d9e1f28e4b6faac490331e5ff9c9",
      "cb262b65e819484abce2a7683a960d7e",
      "47c6cec237b14f59af9a23dfb128088c",
      "ee76fe624a0c4911996a98b6c694f983",
      "1904b5c198ad47c6acd59c9748436d4d",
      "844eb88297154db089651f273e5fbe17"
     ]
    },
    "id": "x1a9JoG2ygeo",
    "outputId": "71f9ed10-b7f7-4e27-9e85-655b4426e828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 50000 | Val samples: 10000 | Classes: 10\n",
      "Loaded finetuned model from drive.\n",
      "Pruned model params: total=2,704,554, trainable=2,704,554\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2ea89dd3ef40aabcbf6d9a35399671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pruned FT Epoch 1:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b3ba1193ece4ea8a1fedb2913597f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned FT Epoch 1 | Loss: 1.0158 | Val Acc: 78.54%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ecad083bf048dd87ee3cde7333581e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pruned FT Epoch 2:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab6d4ba2aaa46a5b94e2db102faa824",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned FT Epoch 2 | Loss: 0.6271 | Val Acc: 80.70%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53ba10f19334b9c8d9cdde4b40725bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pruned FT Epoch 3:   0%|          | 0/391 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2688bc814f1e41a89dfbe52c9904be53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned FT Epoch 3 | Loss: 0.4838 | Val Acc: 84.42%\n",
      "Saved pruned model: /content/drive/MyDrive/checkpoints/resnet18_cifar10_pruned.pth, size=10.88 MB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9222483f62f544e5a590c35a3e6fa63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Pruned Model Metrics ---\n",
      "Total params:        2,704,554\n",
      "Trainable params:    2,704,554\n",
      "Model Size:          10.88 MB\n",
      "Accuracy:            84.42%\n",
      "Latency (CPU, ms):   21.206\n",
      "Latency (GPU, ms): 2.543\n"
     ]
    }
   ],
   "source": [
    "#pruning of model\n",
    "import os, time, copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 8\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "val_ds   = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                          prefetch_factor=4, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                        prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)} | Classes: {NUM_CLASSES}\")\n",
    "\n",
    "# utility fcns\n",
    "def count_parameters(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def save_model_state(m, path):\n",
    "    torch.save(m.state_dict(), path)\n",
    "    return os.path.getsize(path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def latency_ms(model, device, n=50):\n",
    "    model.to(device).eval()\n",
    "    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device)\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    return 1000.0 * (time.time() - t0) / n\n",
    "\n",
    "# model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "# Load finetuned checkpoint\n",
    "ckpt_dir = \"/content/drive/MyDrive/checkpoints\"\n",
    "os.makedirs(ckpt_dir, exist_ok=True)\n",
    "ckpt_path = os.path.join(ckpt_dir, \"finetuned_resnet18_cifar10_final.pth\")\n",
    "model.load_state_dict(torch.load(ckpt_path, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Loaded finetuned model from drive.\")\n",
    "\n",
    "# structured pruning\n",
    "!pip install -q torch-pruning\n",
    "import torch_pruning as tp\n",
    "\n",
    "pruned_model = copy.deepcopy(model)\n",
    "pruned_model.to(device)\n",
    "pruned_model.eval()\n",
    "\n",
    "example_inputs = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "DG = tp.DependencyGraph().build_dependency(pruned_model, example_inputs=example_inputs)\n",
    "\n",
    "prune_ratio = 0.3  # Fraction of channels to remove\n",
    "prunable_layers = [m for m in pruned_model.modules() if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "for layer in prunable_layers:\n",
    "    n_remove = int(prune_ratio * layer.out_channels)\n",
    "    if n_remove <= 0:\n",
    "        continue\n",
    "    weight = layer.weight.data.abs().view(layer.out_channels, -1).sum(1)\n",
    "    prune_idx = torch.argsort(weight)[:n_remove].cpu().tolist()\n",
    "    # Updated API for torch-pruning\n",
    "    group = DG.get_pruning_group(layer, tp.prune_conv_out_channels, prune_idx)\n",
    "    if group is not None:\n",
    "        group.prune()\n",
    "\n",
    "total_params, trainable_params = count_parameters(pruned_model)\n",
    "print(f\"Pruned model params: total={total_params:,}, trainable={trainable_params:,}\")\n",
    "\n",
    "#finetuning the pruned model\n",
    "pruned_model.train()\n",
    "optimizer_pruned = torch.optim.Adam([p for p in pruned_model.parameters() if p.requires_grad], lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_ft_epochs = 3\n",
    "\n",
    "for epoch in range(num_ft_epochs):\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Pruned FT Epoch {epoch+1}\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer_pruned.zero_grad()\n",
    "        out = pruned_model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer_pruned.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss = running_loss / len(train_ds)\n",
    "    val_acc = evaluate(pruned_model, val_loader, device)\n",
    "    print(f\"Pruned FT Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "#save pruned model\n",
    "pruned_path = os.path.join(ckpt_dir, \"resnet18_cifar10_pruned.pth\")\n",
    "saved_size = save_model_state(pruned_model, pruned_path)\n",
    "print(f\"Saved pruned model: {pruned_path}, size={saved_size/1e6:.2f} MB\")\n",
    "\n",
    "#pruned model metrics\n",
    "pruned_model.eval()\n",
    "lat_cpu = latency_ms(pruned_model.cpu(), torch.device(\"cpu\"))\n",
    "lat_gpu = latency_ms(pruned_model.to(device), device) if torch.cuda.is_available() else None\n",
    "pruned_acc = evaluate(pruned_model, val_loader, device)\n",
    "\n",
    "print(\"\\n--- Pruned Model Metrics ---\")\n",
    "print(f\"Total params:        {total_params:,}\")\n",
    "print(f\"Trainable params:    {trainable_params:,}\")\n",
    "print(f\"Model Size:          {saved_size/1e6:.2f} MB\")\n",
    "print(f\"Accuracy:            {pruned_acc*100:.2f}%\")\n",
    "print(f\"Latency (CPU, ms):   {lat_cpu:.3f}\")\n",
    "if lat_gpu: print(f\"Latency (GPU, ms): {lat_gpu:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pruning + qat of the finetuned model\n",
    "import os, time, copy\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "\n",
    "# Mount Drive for checkpoints\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "#dataset\n",
    "BATCH_SIZE = 128\n",
    "IMG_SIZE = 224\n",
    "NUM_WORKERS = 2\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406],[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_ds = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
    "val_ds   = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=val_transform)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                          num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                          prefetch_factor=4, persistent_workers=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=True,\n",
    "                        prefetch_factor=4, persistent_workers=True)\n",
    "\n",
    "print(f\"Train samples: {len(train_ds)} | Val samples: {len(val_ds)} | Classes: {NUM_CLASSES}\")\n",
    "\n",
    "#metric fcns\n",
    "def count_parameters(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "def save_model_state(m, path):\n",
    "    torch.save(m.state_dict(), path)\n",
    "    return os.path.getsize(path)\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in tqdm(loader, desc=\"Eval\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def latency_ms(model, device, n=50):\n",
    "    model.to(device).eval()\n",
    "    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device)\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        _ = model(dummy)\n",
    "        if device.type == \"cuda\": torch.cuda.synchronize()\n",
    "    return 1000.0 * (time.time() - t0) / n\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "model = resnet18(weights=None)\n",
    "model.fc = nn.Linear(model.fc.in_features, NUM_CLASSES)\n",
    "\n",
    "# load finetuned model from checkpoint\n",
    "ckpt_dir = \"/content/drive/MyDrive/checkpoints\"\n",
    "ckpt_path = os.path.join(ckpt_dir, \"finetuned_resnet18_cifar10_final.pth\")\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(\"Loaded finetuned FP32 model.\")\n",
    "\n",
    "#structured pruning\n",
    "!pip install -q torch-pruning\n",
    "import torch_pruning as tp\n",
    "\n",
    "pruned_model = copy.deepcopy(model)\n",
    "pruned_model.to(device)\n",
    "pruned_model.eval()\n",
    "\n",
    "example_inputs_pruning = torch.randn(1, 3, IMG_SIZE, IMG_SIZE).to(device)\n",
    "DG = tp.DependencyGraph().build_dependency(pruned_model, example_inputs=example_inputs_pruning)\n",
    "\n",
    "prune_ratio = 0.3\n",
    "prunable_layers = [m for m in pruned_model.modules() if isinstance(m, nn.Conv2d)]\n",
    "\n",
    "for layer in prunable_layers:\n",
    "    n_remove = int(prune_ratio * layer.out_channels)\n",
    "    if n_remove <= 0:\n",
    "        continue\n",
    "    weight = layer.weight.data.abs().view(layer.out_channels, -1).sum(1)\n",
    "    prune_idx = torch.argsort(weight)[:n_remove].cpu().tolist()\n",
    "    group = DG.get_pruning_group(layer, tp.prune_conv_out_channels, prune_idx)\n",
    "    if group is not None:\n",
    "        group.prune()\n",
    "\n",
    "total_params, trainable_params = count_parameters(pruned_model)\n",
    "print(f\"After pruning: total params={total_params:,}, trainable={trainable_params:,}\")\n",
    "\n",
    "#finetune pruned model\n",
    "pruned_model.train()\n",
    "optimizer = torch.optim.Adam([p for p in pruned_model.parameters() if p.requires_grad], lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "num_ft_epochs = 3\n",
    "\n",
    "for epoch in range(num_ft_epochs):\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"Pruned FT Epoch {epoch+1}\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = pruned_model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss = running_loss / len(train_ds)\n",
    "    val_acc = evaluate(pruned_model, val_loader, device)\n",
    "    print(f\"Pruned FT Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx\n",
    "from torch.ao.quantization.qconfig import get_default_qat_qconfig\n",
    "\n",
    "qat_qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "example_inputs = (torch.randn(1, 3, IMG_SIZE, IMG_SIZE),)\n",
    "\n",
    "# Prepare pruned model for QAT\n",
    "qat_model = copy.deepcopy(pruned_model)\n",
    "qat_prepared = prepare_qat_fx(qat_model, {\"\": qat_qconfig}, example_inputs)\n",
    "qat_prepared.to(device)\n",
    "qat_prepared.train()\n",
    "\n",
    "#finetune qat model\n",
    "qat_epochs = 3\n",
    "qat_lr = 5e-5\n",
    "qat_optimizer = torch.optim.Adam([p for p in qat_prepared.parameters() if p.requires_grad], lr=qat_lr)\n",
    "qat_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(qat_epochs):\n",
    "    running_loss = 0.0\n",
    "    for xb, yb in tqdm(train_loader, desc=f\"QAT Epoch {epoch+1}\", leave=False):\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        qat_optimizer.zero_grad()\n",
    "        out = qat_prepared(xb)\n",
    "        loss = qat_criterion(out, yb)\n",
    "        loss.backward()\n",
    "        qat_optimizer.step()\n",
    "        running_loss += loss.item() * xb.size(0)\n",
    "    epoch_loss = running_loss / len(train_ds)\n",
    "    val_acc = evaluate(qat_prepared, val_loader, device)\n",
    "    print(f\"QAT Epoch {epoch+1} | Loss: {epoch_loss:.4f} | Fake-Quant Val Acc: {val_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "qat_prepared.eval()\n",
    "qat_prepared.cpu()\n",
    "\n",
    "qat_int8 = convert_fx(qat_prepared)\n",
    "\n",
    "\n",
    "\n",
    "int8_path = os.path.join(ckpt_dir, \"resnet18_cifar10_pruned_QAT_int8.pth\")\n",
    "torch.save(qat_int8.state_dict(), int8_path)\n",
    "print(f\"Saved INT8 QAT model → {int8_path}\")\n",
    "\n",
    "\n",
    "val_loader_cpu = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=False,\n",
    ")\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_cpu(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    for xb, yb in loader:\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1)\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "def latency_cpu(model, n=50):\n",
    "    model.eval()\n",
    "    dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE)\n",
    "    for _ in range(10):\n",
    "        _ = model(dummy)\n",
    "    t0 = time.time()\n",
    "    for _ in range(n):\n",
    "        _ = model(dummy)\n",
    "    return 1000 * (time.time() - t0) / n\n",
    "\n",
    "\n",
    "#metrics\n",
    "\n",
    "int8_acc = evaluate_cpu(qat_int8, val_loader_cpu)\n",
    "int8_size = os.path.getsize(int8_path)\n",
    "lat_cpu = latency_cpu(qat_int8)\n",
    "\n",
    "print(\"\\n--- Pruned + QAT INT8 Metrics ---\")\n",
    "print(f\"Total params:        {total_params:,}\")\n",
    "print(f\"Trainable params:    {trainable_params:,}\")\n",
    "print(f\"Model Size:          {int8_size/1e6:.2f} MB\")\n",
    "print(f\"Accuracy:            {int8_acc*100:.2f}%\")\n",
    "print(f\"Latency (CPU, ms):   {lat_cpu:.3f}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}